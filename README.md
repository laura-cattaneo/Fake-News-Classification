# Classifying Fake News

The spread of misinformation threatens businesses and their reputations worldwide. A 2019 report from the University of Baltimore estimates that companies spend over $9.5B to combat the negative impact of misinformation on their brands annually. To limit 1 the spread of misinformation and mitigate its negative impact, companies need to find and debunk fake news as quickly as possible. For that reason, we explore the ability of data and analytics to address this issue. Specifically, we aim to answer the following questions:

1. Can we build a classification model using data collected from the internet to predict whether or not a publication is fake     news?
2. How well does the model classify news as real or fake?
3. How could companies implement a predictive model to mitigate the negative impact of misinformation on their business?
  
This project was part of the class 'Business Essentials' that I took in my first semester as a University of Minnesota Master in Business Analytics student. 
I worked with [Callie Page](https://www.linkedin.com/in/callie-page-ds1/), [Casey Easterday](http://linkedin.com/in/caeasterday), [Jessica Lin](https://www.linkedin.com/in/tzu-hsuan-jessica/), and [Laura Catano](https://www.linkedin.com/in/laura-catano-a9a8a0b9). 
We extracted titles from the subreddits r/TheOnion and r/nottheonion, and built a classifier to determine if an article is "fake" or not based on the title. We tried techniques such as Count Vectorizer, TF-IDF, Random Forest, SVM, and Naive Bayes. In addition, we also tested our classifier on an [external Kaggle dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset).

Check out our video explaining our work and the results: https://www.youtube.com/watch?v=gkj57GjQOns 
